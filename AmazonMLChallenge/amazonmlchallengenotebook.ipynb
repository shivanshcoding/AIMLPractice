{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14462962,"sourceType":"datasetVersion","datasetId":8454287},{"sourceId":716170,"sourceType":"modelInstanceVersion","modelInstanceId":544398,"modelId":557481}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:03.585894Z","iopub.execute_input":"2026-01-12T11:51:03.586653Z","iopub.status.idle":"2026-01-12T11:51:03.597090Z","shell.execute_reply.started":"2026-01-12T11:51:03.586623Z","shell.execute_reply":"2026-01-12T11:51:03.596373Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/models/pytorch/models-by-shivansh/1/lgbm_regressor.pkl\n/kaggle/input/models/pytorch/models-by-shivansh/1/price_processor.joblib\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/config.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/merges.txt\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/training_args.bin\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/vocab.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/tokenizer_config.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/model.safetensors\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/special_tokens_map.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/config.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/trainer_state.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/training_args.bin\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/scaler.pt\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/scheduler.pt\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/model.safetensors\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/optimizer.pt\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-1172/rng_state.pth\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/config.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/trainer_state.json\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/training_args.bin\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/scaler.pt\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/scheduler.pt\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/model.safetensors\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/optimizer.pt\n/kaggle/input/models/pytorch/models-by-shivansh/1/roberta_classifier/checkpoint-2344/rng_state.pth\n/kaggle/input/amazon-ml-challenge/sample_test.csv\n/kaggle/input/amazon-ml-challenge/sample_test_out.csv\n/kaggle/input/amazon-ml-challenge/evaluation.py\n/kaggle/input/amazon-ml-challenge/train.csv\n/kaggle/input/amazon-ml-challenge/test.csv\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import scipy, transformers, accelerate, sklearn, joblib, re, os\nfrom scipy import stats\nfrom scipy.stats.mstats import winsorize\nimport lightgbm as lgb\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:03.598466Z","iopub.execute_input":"2026-01-12T11:51:03.598665Z","iopub.status.idle":"2026-01-12T11:51:03.609758Z","shell.execute_reply.started":"2026-01-12T11:51:03.598645Z","shell.execute_reply":"2026-01-12T11:51:03.609155Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import shutil\nimport os\n\nSRC_MODEL_DIR = \"/kaggle/input/models/pytorch/models-by-shivansh/1\"\nDST_MODEL_DIR = \"/kaggle/working/models\"\n\nos.makedirs(DST_MODEL_DIR, exist_ok=True)\n\nfor item in os.listdir(SRC_MODEL_DIR):\n    src = os.path.join(SRC_MODEL_DIR, item)\n    dst = os.path.join(DST_MODEL_DIR, item)\n\n    if os.path.isdir(src):\n        shutil.copytree(src, dst, dirs_exist_ok=True)\n    else:\n        shutil.copy2(src, dst)\n\nprint(\"âœ… Models copied to writable directory\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:03.610734Z","iopub.execute_input":"2026-01-12T11:51:03.611177Z","iopub.status.idle":"2026-01-12T11:51:07.817742Z","shell.execute_reply.started":"2026-01-12T11:51:03.611153Z","shell.execute_reply":"2026-01-12T11:51:07.816750Z"}},"outputs":[{"name":"stdout","text":"âœ… Models copied to writable directory\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 \\\n  --index-url https://download.pytorch.org/whl/cu121\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:07.818665Z","iopub.execute_input":"2026-01-12T11:51:07.818892Z","iopub.status.idle":"2026-01-12T11:51:47.990516Z","shell.execute_reply.started":"2026-01-12T11:51:07.818836Z","shell.execute_reply":"2026-01-12T11:51:47.989590Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.2.2+cu121\nUninstalling torch-2.2.2+cu121:\n  Successfully uninstalled torch-2.2.2+cu121\nFound existing installation: torchvision 0.17.2+cu121\nUninstalling torchvision-0.17.2+cu121:\n  Successfully uninstalled torchvision-0.17.2+cu121\nFound existing installation: torchaudio 2.2.2+cu121\nUninstalling torchaudio-2.2.2+cu121:\n  Successfully uninstalled torchaudio-2.2.2+cu121\nLooking in indexes: https://download.pytorch.org/whl/cu121\nCollecting torch==2.2.2\n  Using cached https://download.pytorch.org/whl/cu121/torch-2.2.2%2Bcu121-cp312-cp312-linux_x86_64.whl (757.2 MB)\nCollecting torchvision==0.17.2\n  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\nCollecting torchaudio==2.2.2\n  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (4.15.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.2) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.2) (11.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2) (3.0.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2) (1.3.0)\nInstalling collected packages: torch, torchvision, torchaudio\nSuccessfully installed torch-2.2.2+cu121 torchaudio-2.2.2+cu121 torchvision-0.17.2+cu121\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nprint(torch.__version__)\nprint(torch.version.cuda)\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:47.992726Z","iopub.execute_input":"2026-01-12T11:51:47.993077Z","iopub.status.idle":"2026-01-12T11:51:47.998243Z","shell.execute_reply.started":"2026-01-12T11:51:47.993040Z","shell.execute_reply":"2026-01-12T11:51:47.997450Z"}},"outputs":[{"name":"stdout","text":"2.8.0+cu126\n12.6\nTrue\nTesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"# --- File Paths ---\nDATA_DIR = \"/kaggle/input/amazon-ml-challenge/\"\nTRAIN_FILE = f\"{DATA_DIR}train.csv\"\nTEST_FILE = f\"{DATA_DIR}test.csv\"\nMODEL_DIR = \"/kaggle/working/models/\"\nSUBMISSION_FILE = \"/kaggle/working/submission.csv\"\n\n# --- Preprocessing ---\nPRICE_COLUMN = \"price\"\nTEXT_COLUMN = \"catalog_content\"\nTARGET_CLASS_COLUMN = \"price_class\"\nNUM_PRICE_CLASSES = 14\n\n# --- RoBERTa Model ---\nROBERTA_MODEL_NAME = \"roberta-base\"\nROBERTA_MODEL_PATH = f\"{MODEL_DIR}roberta_classifier\"\nROBERTA_BATCH_SIZE = 64      # Increased for speed\nROBERTA_EPOCHS = 2           # Reduced, often sufficient\nROBERTA_LEARNING_RATE = 2e-5\n\n# --- Ensemble Model (LightGBM) ---\nENSEMBLE_MODEL_PATH = f\"{MODEL_DIR}lgbm_regressor.pkl\"\nLGBM_PARAMS = {\n    'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 2000,\n    'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n    'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1,\n    'num_leaves': 31, 'verbose': -1, 'n_jobs': -1,\n}\nSVM_PARAMS = {'kernel': 'rbf', 'C': 1.0, 'epsilon': 0.1}\nDT_PARAMS = {'max_depth': 10, 'min_samples_split': 5}\nADABOOST_PARAMS = {'n_estimators': 50, 'learning_rate': 1.0}\nVOTING_WEIGHTS = [0.3, 0.3, 0.4] # Adjust these based on your preference\n\n# --- General ---\nRANDOM_STATE = 42\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:47.999158Z","iopub.execute_input":"2026-01-12T11:51:47.999359Z","iopub.status.idle":"2026-01-12T11:51:48.019599Z","shell.execute_reply.started":"2026-01-12T11:51:47.999338Z","shell.execute_reply":"2026-01-12T11:51:48.018910Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"**Data PreProcessing**","metadata":{}},{"cell_type":"code","source":"# ----------------- Feature Engineering (from your provided code) -----------------\nweight_units = {'g','gram','grams','gramm','gr','kg','k','lb','pound','pounds','oz','ounce','ounces'}\nvolume_units = {'ml','millilitre','milliliter','mililitro','ltr','liter','liters','l','fl oz','fluid ounce','fluid ounces','fluid ounces'}\ncount_units  = {'count','ct','each','unit','units','pack','packs','piece','pieces','box','bottle','bottles','bag','bags','case','carton','capsule','jar','pouch','bucket','tea bags','paper cupcake liners'}\nlength_units = {'cm','mm','m','meter','meters','inch','inches','in','ft','foot','feet'}\n\ndef extract_value_unit(text):\n    lines = [l.strip() for l in str(text).split(\"\\n\") if l.strip()]\n    if len(lines) < 2:\n        return pd.Series([None, None])\n    value_line = lines[-2]\n    unit_line = lines[-1]\n    match_val = re.search(r\"[\\d\\.]+\", value_line)\n    value = float(match_val.group()) if match_val else None\n    unit = re.sub(r'Unit[s]?:\\s*', '', unit_line, flags=re.IGNORECASE).strip()\n    return pd.Series([value, unit])\n\ndef extract_ipq(text):\n    patterns = [r'pack of (\\d+)', r'(\\d+)\\s*count', r'box of (\\d+)', r'case of (\\d+)', r'(\\d+)\\s*pieces', r'(\\d+)\\s*units', r'(\\d+)\\s*ct', r'(\\d+)\\s*each']\n    text_lower = str(text).lower()\n    for pat in patterns:\n        match = re.search(pat, text_lower)\n        if match:\n            return int(match.group(1))\n    return 1\n\ndef map_unit_class(unit):\n    if unit is None: return 'others'\n    u = unit.strip().lower()\n    if u in weight_units: return 'weight'\n    if u in volume_units: return 'volume'\n    if u in count_units: return 'count'\n    if u in length_units: return 'length'\n    return 'others'\n\ndef convert_to_standard(value, unit):\n    if value is None or unit is None: return value\n    u = unit.strip().lower()\n    if u in {'oz', 'ounce', 'ounces'}: return value * 28.3495\n    if u in {'lb', 'pound', 'pounds'}: return value * 453.592\n    if u in {'kg', 'k'}: return value * 1000\n    if u in {'g', 'gram', 'grams', 'gramm', 'gr'}: return value\n    if u in {'ltr', 'liter', 'liters', 'l'}: return value * 1000\n    if u in {'fl oz', 'fluid ounce', 'fluid ounces'}: return value * 29.5735\n    if u in {'ml', 'millilitre', 'milliliter', 'mililitro'}: return value\n    if u in {'cm'}: return value / 100\n    if u in {'mm'}: return value / 1000\n    if u in {'inch', 'inches', 'in'}: return value * 0.0254\n    if u in {'ft', 'foot', 'feet'}: return value * 0.3048\n    if u in {'m', 'meter', 'meters'}: return value\n    if u in count_units: return value\n    return value\n\ndef engineer_features(df):\n    \"\"\"Applies all feature engineering steps.\"\"\"\n    temp_df = df.copy()\n    temp_df[['value', 'unit']] = temp_df['catalog_content'].apply(extract_value_unit)\n    temp_df['IPQ'] = temp_df['catalog_content'].apply(extract_ipq)\n    temp_df['standardised_units'] = temp_df['unit'].apply(map_unit_class)\n    temp_df['updated_values'] = temp_df.apply(lambda row: convert_to_standard(row['value'], row['unit']), axis=1)\n    \n    # Fill NaNs created during feature engineering\n    temp_df['updated_values'] = temp_df['updated_values'].fillna(temp_df['updated_values'].median())\n    temp_df['IPQ'] = temp_df['IPQ'].fillna(1)\n    \n    # One-hot encode and ensure all possible columns are present\n    unit_dummies = pd.get_dummies(temp_df['standardised_units'], prefix='unit')\n    for col in ['unit_weight', 'unit_volume', 'unit_count', 'unit_length', 'unit_others']:\n        if col not in unit_dummies.columns:\n            unit_dummies[col] = 0\n    temp_df = pd.concat([temp_df, unit_dummies], axis=1)\n    \n    return temp_df\n\n# ----------------- Price Transformation Pipeline -----------------\n\nclass PriceProcessor:\n    \"\"\"Handles Box-Cox, Winsorization, and Binning for the price column.\"\"\"\n    def __init__(self):\n        self.lambda_ = None\n        self.winsor_limits = (0.01, 0.01) # Winsorize 1% from both tails\n        self.bin_edges = None\n        self.path = os.path.join(MODEL_DIR, \"price_processor.joblib\")\n\n    def fit_transform(self, price_series):\n        # 1. Box-Cox Transform (add 1 to handle prices <= 0)\n        transformed_price, self.lambda_ = stats.boxcox(price_series + 1)\n        \n        # 2. Winsorization\n        winsorized_price = winsorize(transformed_price, limits=self.winsor_limits)\n        \n        # 3. Binning into classes\n        binned_price, self.bin_edges = pd.qcut(\n            winsorized_price, \n            q=NUM_PRICE_CLASSES, \n            labels=False, \n            retbins=True, \n            duplicates='drop'\n        )\n        self.save()\n        return binned_price\n\n    def save(self):\n        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n        joblib.dump(self, self.path)\n\n    @classmethod\n    def load(cls):\n        return joblib.load(os.path.join(MODEL_DIR, \"price_processor.joblib\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:48.020365Z","iopub.execute_input":"2026-01-12T11:51:48.020551Z","iopub.status.idle":"2026-01-12T11:51:48.038630Z","shell.execute_reply.started":"2026-01-12T11:51:48.020531Z","shell.execute_reply":"2026-01-12T11:51:48.038070Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"**Evaluation**","metadata":{}},{"cell_type":"code","source":"def smape(y_true, y_pred):\n    # Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    \n    # Handle the case where both true and pred are zero\n    # to avoid division by zero\n    ratio = np.where(denominator == 0, 0, numerator / denominator)\n    \n    return np.mean(ratio) * 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:48.039332Z","iopub.execute_input":"2026-01-12T11:51:48.039552Z","iopub.status.idle":"2026-01-12T11:51:48.054595Z","shell.execute_reply.started":"2026-01-12T11:51:48.039529Z","shell.execute_reply":"2026-01-12T11:51:48.054011Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"**Ensemble Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, VotingRegressor\nfrom sklearn.impute import SimpleImputer\n\ndef get_feature_columns(df):\n    \"\"\"Identifies the feature columns for the ensemble model.\"\"\"\n    exclude_cols = [\n        'sample_id', 'catalog_content', 'image_link', 'price',\n        'price_class', 'value', 'unit', 'standardised_units'\n    ]\n    feature_cols = [col for col in df.columns if col not in exclude_cols]\n    # print(f\"DEBUG: Selected {len(feature_cols)} numeric features: {feature_cols}\")\n    return feature_cols\n\ndef train_ensemble_model(df, target_col):\n    \"\"\"Trains and saves the Voting Regressor model (SVR + DecisionTree + AdaBoost).\"\"\"\n    train_split, val_split = train_test_split(df, test_size=0.2, random_state=42)\n    \n    feature_cols = get_feature_columns(df)\n    X_train = train_split[feature_cols]\n    y_train = train_split[target_col]\n\n    X_val = val_split[feature_cols]\n    y_val = val_split[target_col]\n\n    # Base learners\n    svr = make_pipeline(StandardScaler(), SVR(**SVM_PARAMS))\n    dtr = DecisionTreeRegressor(random_state=RANDOM_STATE, **DT_PARAMS)\n    ada = AdaBoostRegressor(random_state=RANDOM_STATE, **ADABOOST_PARAMS)\n\n    # Voting ensemble\n    model = VotingRegressor(\n        estimators=[\n            (\"svr\", svr),\n            (\"dt\", dtr),\n            (\"ada\", ada),\n        ],\n        weights=VOTING_WEIGHTS,\n        n_jobs=None,  # VotingRegressor in sklearn doesn't support n_jobs (only in some versions for estimators); keep None\n    )\n\n    model.fit(X_train, y_train)\n    val_predictions = model.predict(X_val)\n    train_predictions = model.predict(X_train)\n\n    score_train = smape(y_train,train_predictions)\n    score_val = smape(y_val, val_predictions)\n    \n    print(f\"\\nðŸ“‰ Validation SMAPE Score: {score_val:.4f}%\")\n    print(f\"\\nðŸ“‰ Training SMAPE Score: {score_train:.4f}%\")\n    print(\"----------------------------------------------\\n\")\n\n    # Save the model\n    os.makedirs(os.path.dirname(ENSEMBLE_MODEL_PATH), exist_ok=True)\n    joblib.dump(model, ENSEMBLE_MODEL_PATH)\n    print(f\"Ensemble model saved to {ENSEMBLE_MODEL_PATH}\")\n    return model\n\ndef predict_with_ensemble_model(df):\n    \"\"\"Loads and predicts with the saved Voting Regressor model.\"\"\"\n    feature_cols = get_feature_columns(df)\n    X_test = df[feature_cols]\n\n    model = joblib.load(ENSEMBLE_MODEL_PATH)\n    predictions = model.predict(X_test)\n    return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:48.055450Z","iopub.execute_input":"2026-01-12T11:51:48.055905Z","iopub.status.idle":"2026-01-12T11:51:48.070684Z","shell.execute_reply.started":"2026-01-12T11:51:48.055852Z","shell.execute_reply":"2026-01-12T11:51:48.069908Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"**Roberta Classifier**","metadata":{}},{"cell_type":"code","source":"class PriceDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef train_roberta(train_df, text_col, target_col):\n    \"\"\"Fine-tunes and saves the RoBERTa model.\"\"\"\n    tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n    model = RobertaForSequenceClassification.from_pretrained(\n        ROBERTA_MODEL_NAME,\n        num_labels=train_df[target_col].nunique()\n    ).to(DEVICE)\n\n    train_dataset = PriceDataset(\n        texts=train_df[text_col].to_numpy(),\n        labels=train_df[target_col].to_numpy(),\n        tokenizer=tokenizer\n    )\n\n    training_args = TrainingArguments(\n        output_dir=ROBERTA_MODEL_PATH,\n        num_train_epochs=ROBERTA_EPOCHS,\n        per_device_train_batch_size=ROBERTA_BATCH_SIZE,\n        learning_rate=ROBERTA_LEARNING_RATE,\n        logging_dir='./logs',\n        logging_steps=100,\n        do_train=True,\n        do_eval=False,\n        save_strategy=\"epoch\",\n        fp16=True,  # Mixed-precision for speed\n        load_best_model_at_end=False,\n        seed=RANDOM_STATE,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n    )\n\n    trainer.train()\n    trainer.save_model(ROBERTA_MODEL_PATH)\n    tokenizer.save_pretrained(ROBERTA_MODEL_PATH)\n    print(f\"RoBERTa model saved to {ROBERTA_MODEL_PATH}\")\n\nimport gc\n\ndef predict_with_roberta(df, text_col, chunk_size=5000):\n    \"\"\"\n    Optimized prediction:\n    1. Uses FP16 and Batch Size 64 (Speed).\n    2. Processes in chunks to prevent OOM (Memory).\n    3. Clears cache aggressively.\n    \"\"\"\n    print(f\"ðŸ”„ Initializing Inference on {len(df)} rows...\")\n    \n    tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_PATH)\n    model = RobertaForSequenceClassification.from_pretrained(ROBERTA_MODEL_PATH).to(DEVICE)\n    \n    # optimized arguments for inference\n    test_args = TrainingArguments(\n        output_dir=os.path.join(MODEL_DIR, \"inference_temp\"),\n        per_device_eval_batch_size=64,   # Match your training batch size\n        fp16=True,                       # Much faster, less memory\n        dataloader_num_workers=2,        # Speed up data loading\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(model=model, args=test_args)\n    \n    all_predictions = []\n    \n    # Process in chunks to avoid RAM explosion\n    total_chunks = (len(df) // chunk_size) + 1\n    \n    for i in range(0, len(df), chunk_size):\n        chunk_df = df.iloc[i : i + chunk_size]\n        \n        if chunk_df.empty:\n            continue\n            \n        print(f\"   Processing chunk {i//chunk_size + 1}/{total_chunks} ({len(chunk_df)} rows)...\")\n        \n        # Create dataset just for this chunk\n        chunk_dataset = PriceDataset(\n            texts=chunk_df[text_col].to_numpy(),\n            labels=np.zeros(len(chunk_df)), # Dummy labels\n            tokenizer=tokenizer\n        )\n        \n        # Predict\n        predictions = trainer.predict(chunk_dataset).predictions\n        \n        # Get class indices and append to list\n        chunk_preds = np.argmax(predictions, axis=1)\n        all_predictions.append(chunk_preds)\n        \n        # CRITICAL: Free memory\n        del chunk_dataset, predictions, chunk_preds\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    print(\"âœ… Inference complete.\")\n    return np.concatenate(all_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:48.071611Z","iopub.execute_input":"2026-01-12T11:51:48.071824Z","iopub.status.idle":"2026-01-12T11:51:48.090973Z","shell.execute_reply.started":"2026-01-12T11:51:48.071802Z","shell.execute_reply":"2026-01-12T11:51:48.090342Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def get_roberta_predictions_with_cache(\n    df,\n    text_col,\n    cache_name,\n):\n    \"\"\"\n    Computes RoBERTa predictions once and caches them.\n    On reruns, loads directly from disk.\n    \"\"\"\n    CACHE_DIR = \"/kaggle/working/roberta_cache\"\n    os.makedirs(CACHE_DIR, exist_ok=True)\n\n    cache_path = os.path.join(CACHE_DIR, f\"{cache_name}.npy\")\n\n    if os.path.exists(cache_path):\n        print(f\"âœ… Loading cached RoBERTa predictions from {cache_path}\")\n        return np.load(cache_path)\n\n    print(f\"ðŸš€ Running RoBERTa inference for {cache_name} (this may take time)...\")\n    preds = predict_with_roberta(df, text_col)\n\n    np.save(cache_path, preds)\n    print(f\"ðŸ’¾ Saved RoBERTa predictions to {cache_path}\")\n\n    return preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:48.091698Z","iopub.execute_input":"2026-01-12T11:51:48.092279Z","iopub.status.idle":"2026-01-12T11:51:48.108233Z","shell.execute_reply.started":"2026-01-12T11:51:48.092254Z","shell.execute_reply":"2026-01-12T11:51:48.107575Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"**Main**","metadata":{}},{"cell_type":"code","source":"def main():\n    #--- 0. CRITICAL: Verify GPU is available ---\n    if not torch.cuda.is_available():\n        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n        print(\"!!! ERROR: No CUDA-enabled GPU found !!!\")\n        print(\"!!!       Aborting training.       !!!\")\n        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n        return\n    print(f\"PyTorch confirmed CUDA is available. Using device: {DEVICE.upper()}\")\n    \n    # --- 1. Load Data ---\n    print(\"\\nStep 1: Loading data...\")\n    train_df = pd.read_csv(TRAIN_FILE, nrows = 2000)\n    test_df = pd.read_csv(TEST_FILE, nrows = 2000)\n    print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n\n    # --- 2. Preprocess Price Column for Training ---\n    print(\"\\nStep 2: Preprocessing price column for RoBERTa training...\")\n    price_processor = PriceProcessor()\n    train_df[TARGET_CLASS_COLUMN] = price_processor.fit_transform(train_df[PRICE_COLUMN])\n    print(f\"Price column transformed and binned into {train_df[TARGET_CLASS_COLUMN].nunique()} classes.\")\n\n    # --- 3. Fine-Tune RoBERTa Classifier ---\n    print(\"\\nStep 3: Fine-tuning RoBERTa for price classification...\")\n    model_config_path = os.path.join(ROBERTA_MODEL_PATH, \"config.json\")\n    if not os.path.exists(model_config_path):\n        print(\"RoBERTa model not found. Starting training...\")\n        train_roberta(train_df, TEXT_COLUMN, TARGET_CLASS_COLUMN)\n    else:\n        print(\"Found existing RoBERTa model. Skipping training.\")\n\n    # --- 4. Get RoBERTa Predictions as Features ---\n    print(\"\\nStep 4: Generating price class predictions from RoBERTa (with caching)...\")\n\n    train_df['roberta_pred_class'] = predict_with_roberta(train_df, TEXT_COLUMN)\n    test_df['roberta_pred_class'] = predict_with_roberta(test_df, TEXT_COLUMN)\n\n    print(\"âœ… RoBERTa predictions added as features.\")\n\n    # --- 5. Engineer Additional Features ---\n    print(\"\\nStep 5: Engineering features from catalog_content...\")\n    train_df_featured = engineer_features(train_df)\n    test_df_featured = engineer_features(test_df)\n    print(\"Feature engineering complete.\")\n\n    # --- 6. Train Ensemble Model ---\n    print(\"\\nStep 6: Training the final LightGBM ensemble model...\")\n    train_ensemble_model(train_df_featured, PRICE_COLUMN)\n\n    # flag = input(\"Do you want to use this model for test predictions?(Y/N) : \")\n    # if(flag in 'Nn'):\n    #     return\n    return\n    # --- 7. Make Final Price Predictions ---\n    print(\"\\nStep 7: Making final price predictions on the test set...\")\n    test_predictions = predict_with_ensemble_model(test_df_featured)\n    test_df['price'] = test_predictions\n\n    # --- 8. Save Results ---\n    # The evaluation step has been removed as test.csv does not have a 'price' column.\n    print(\"\\nStep 8: Saving submission file...\")\n    submission_df = test_df[['sample_id', 'price']]\n    submission_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"âœ… Submission file saved to {SUBMISSION_FILE}\")\n\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T11:51:48.108993Z","iopub.execute_input":"2026-01-12T11:51:48.109216Z","iopub.status.idle":"2026-01-12T11:52:05.302728Z","shell.execute_reply.started":"2026-01-12T11:51:48.109194Z","shell.execute_reply":"2026-01-12T11:52:05.301934Z"}},"outputs":[{"name":"stdout","text":"PyTorch confirmed CUDA is available. Using device: CUDA\n\nStep 1: Loading data...\nTrain shape: (2000, 4), Test shape: (2000, 3)\n\nStep 2: Preprocessing price column for RoBERTa training...\nPrice column transformed and binned into 14 classes.\n\nStep 3: Fine-tuning RoBERTa for price classification...\nFound existing RoBERTa model. Skipping training.\n\nStep 4: Generating price class predictions from RoBERTa (with caching)...\nðŸ”„ Initializing Inference on 2000 rows...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:4824: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n","output_type":"stream"},{"name":"stdout","text":"   Processing chunk 1/1 (2000 rows)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"âœ… Inference complete.\nðŸ”„ Initializing Inference on 2000 rows...\n   Processing chunk 1/1 (2000 rows)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"âœ… Inference complete.\nâœ… RoBERTa predictions added as features.\n\nStep 5: Engineering features from catalog_content...\nFeature engineering complete.\n\nStep 6: Training the final LightGBM ensemble model...\n\nðŸ“‰ Validation SMAPE Score: 53.1322%\n\nðŸ“‰ Training SMAPE Score: 51.9080%\n----------------------------------------------\n\nEnsemble model saved to /kaggle/working/models/lgbm_regressor.pkl\n","output_type":"stream"}],"execution_count":35}]}